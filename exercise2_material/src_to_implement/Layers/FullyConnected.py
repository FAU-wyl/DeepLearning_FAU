import numpy as np
from Layers.Base import BaseLayer


class FullyConnected(BaseLayer):
    def __init__(self, input_size, output_size):
        """
        å…¨è¿æ¥å±‚æ„é€ å‡½æ•°
        :param input_size: è¾“å…¥ç‰¹å¾çš„æ•°é‡ (fan_in)
        :param output_size: è¾“å‡ºç‰¹å¾çš„æ•°é‡ (fan_out)
        """
        super().__init__()
        self.trainable = True

        # å­˜å‚¨ fan_in å’Œ fan_out ä»¥ä¾¿åˆå§‹åŒ–å™¨ä½¿ç”¨
        self.fan_in = input_size
        self.fan_out = output_size

        # åˆå§‹é»˜è®¤ä½¿ç”¨ UniformRandom åˆå§‹åŒ–
        self.weights = np.random.uniform(0, 1, (input_size + 1, output_size))

        self._optimizer = None
        self.input_tensor = None
        self._gradient_weights = None

    # ... [forward, optimizer getter/setter, gradient_weights property ä¿æŒä¸å˜] ...

    def forward(self, input_tensor):
        # Get batch_size, represents the number of inputs processed simultaneously.
        batch_size = input_tensor.shape[0]

        # input_tensor shape: (batch_size, input_size) -> (batch_size, input_size + 1)
        bias_column = np.ones((batch_size, 1))

        # Add bias column (column of 1) to input_tensor(åœ¨è¾“å…¥çŸ©é˜µå³ä¾§æ‹¼æ¥ä¸€åˆ—å…¨ 1ï¼‰
        self.input_tensor = np.concatenate((input_tensor, bias_column), axis=1)

        # Y = X * W
        output_tensor = np.dot(self.input_tensor, self.weights)
        return output_tensor

    @property
    def optimizer(self):
        return self._optimizer

    @optimizer.setter
    def optimizer(self, optimizer):
        self._optimizer = optimizer

    def backward(self, error_tensor):
        self._gradient_weights = np.dot(self.input_tensor.T, error_tensor)

        if self._optimizer is not None:
            self.weights = self._optimizer.calculate_update(self.weights, self._gradient_weights)

        pre_layer_error_tensor = np.dot(error_tensor, self.weights.T)
        # ç§»é™¤åç½®é¡¹çš„æ¢¯åº¦ï¼ˆæœ€åä¸€è¡Œï¼‰
        return pre_layer_error_tensor[:, :-1]

    @property
    def gradient_weights(self):
        return self._gradient_weights

    # =========================================================================
    # ğŸŒŸ æ–°å¢/æ›´æ–°æ–¹æ³•ï¼šinitialize (çº¯ NumPy å®ç°)
    # =========================================================================
    def initialize(self, weights_initializer, bias_initializer):
        """
        ä½¿ç”¨æä¾›çš„åˆå§‹åŒ–å™¨é‡æ–°åˆå§‹åŒ–ä¸»æƒé‡ W å’Œåç½® Bã€‚
        """

        # 1. åˆå§‹åŒ– ä¸»æƒé‡ W
        # W çš„å½¢çŠ¶æ˜¯ (fan_in, fan_out)
        weights_shape_W = (self.fan_in, self.fan_out)

        # è°ƒç”¨åˆå§‹åŒ–å™¨ï¼Œè¿”å› NumPy æ•°ç»„
        W = weights_initializer.initialize(weights_shape_W, self.fan_in, self.fan_out)

        # 2. åˆå§‹åŒ– åç½® B
        # B çš„å½¢çŠ¶æ˜¯ (1, fan_out)
        weights_shape_B = (1, self.fan_out)

        # åç½®é€šå¸¸ fan_in è®¾ä¸º 1
        B = bias_initializer.initialize(weights_shape_B, 1, self.fan_out)

        # 3. å°† W å’Œ B æ‹¼æ¥å› self.weights (B ä½œä¸ºæœ€åä¸€è¡Œ)
        self.weights = np.concatenate((W, B), axis=0)

